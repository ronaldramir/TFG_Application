import streamlit as st

st.set_page_config(
    page_title="Web Scraping | Adquisici√≥n de datos",
    page_icon="üï∑Ô∏è",
    layout="centered"
)

# ------------------------------------------------------------
# HERO
# ------------------------------------------------------------
with st.container(border=True):
    st.title("üï∑Ô∏è Origen y adquisici√≥n de los datos")
    st.caption("Web scraping controlado sobre anuncios p√∫blicos de Crautos.com (veh√≠culos usados)")

st.write("")

# ------------------------------------------------------------
# Fuente de datos
# ------------------------------------------------------------
with st.container(border=True):
    st.header("üåê Fuente de datos")
    st.markdown("""
Los datos provienen del sitio web p√∫blico **Crautos.com**, espec√≠ficamente de su secci√≥n de veh√≠culos usados.  
La extracci√≥n captura los atributos visibles en los anuncios: datos t√©cnicos del veh√≠culo, variables comerciales del anuncio y atributos de ubicaci√≥n/presentaci√≥n.
""")

st.write("")

# ------------------------------------------------------------
# Estrategia de recolecci√≥n
# ------------------------------------------------------------
with st.container(border=True):
    st.header("üß© Estrategia de recolecci√≥n")
    st.markdown("""
Para mejorar la estabilidad operativa, se implement√≥ scraping **controlado y segmentado por rangos de a√±os** (corridas independientes por a√±o o rangos peque√±os).  
Esto permiti√≥:
- Reducir la carga por corrida y mejorar estabilidad  
- Facilitar reintentos y reanudaci√≥n ante fallos  
- Minimizar p√©rdidas de progreso por errores temporales del sitio o del navegador  

Adicionalmente, se restringi√≥ la b√∫squeda a usados (`newused = 0`) para mantener consistencia con el objetivo del proyecto.
""")

st.write("")

# ------------------------------------------------------------
# Herramientas y configuraci√≥n t√©cnica
# ------------------------------------------------------------
with st.container(border=True):
    st.header("üõ†Ô∏è Herramientas y configuraci√≥n t√©cnica")
    st.markdown("""
La extracci√≥n se desarroll√≥ en **Python** utilizando **Selenium** con **Microsoft Edge WebDriver**.  
El flujo automatizado incluye: carga del formulario, selecci√≥n de filtros, manejo de paginaci√≥n y extracci√≥n de detalle abriendo el anuncio en una pesta√±a nueva.
""")

st.write("")

# ------------------------------------------------------------
# Flujo de extracci√≥n (alto nivel)
# ------------------------------------------------------------
with st.container(border=True):
    st.header("üîÅ Flujo de extracci√≥n (alto nivel)")
    st.markdown("""
- Ejecutar b√∫squeda con filtros (rango de a√±os + condici√≥n ‚Äúusados‚Äù)  
- Iterar sobre la paginaci√≥n hasta la √∫ltima p√°gina  
- Por cada card: obtener `car_id` y URL, abrir detalle en nueva pesta√±a, extraer encabezado y tabla, cerrar y volver  
- Consolidar registros y exportar CSV final deduplicado por `car_id`  
""")

st.write("")

# ------------------------------------------------------------
# Variables capturadas
# ------------------------------------------------------------
with st.container(border=True):
    st.header("üßæ Variables capturadas")
    st.markdown("""
Cada anuncio se almacena como una fila con variables del:

**Encabezado:** t√≠tulo, marca, modelo, a√±o (cuando aplica), precio CRC y USD (si est√° disponible).  
**Tabla:** cilindrada, estilo, combustible, transmisi√≥n, estado, kilometraje, colores, puertas, pasajeros, provincia, fecha de ingreso e indicadores comerciales, entre otras.  

Adem√°s, se registran variables t√©cnicas para auditor√≠a y control del proceso: `car_id`, `detail_url`, `pagina`, `posicion_en_pagina`.
""")

st.write("")

# ------------------------------------------------------------
# Estabilidad / robustez
# ------------------------------------------------------------
with st.container(border=True):
    st.header("üßØ Estabilidad, errores y contingencias")
    st.markdown("""
Se incorporaron mecanismos de robustez:
- Reintentos con backoff y jitter ante fallos de carga/HTTP 500  
- Control de ritmo con pausas entre detalles y p√°ginas  
- Checkpoints peri√≥dicos para reanudar desde el √∫ltimo punto exacto  
- Auto-restart del driver ante fallos de sesi√≥n  
- Deduplicaci√≥n final por `car_id`
""")

st.write("")

# ------------------------------------------------------------
# SNIPPETS DE TU C√ìDIGO (REALES)
# ------------------------------------------------------------
with st.container(border=True):
    st.header("üíª Snippets del c√≥digo")
    st.caption("Extractos reales del script de scraping (configuraci√≥n, b√∫squeda y robustez).")

    with st.expander("1) Configuraci√≥n (rango de a√±os, ritmo, reintentos y checkpoints)", expanded=False):
        st.code(
            """# ---------------- CONFIG ----------------
URL = "https://crautos.com/autosusados/index.cfm"

YEAR_FROM = "2008"
YEAR_TO   = "2009"
NEWUSED   = "0"  # 0 = Solo usados

SLEEP_BETWEEN_DETAILS = 1.8
SLEEP_BETWEEN_PAGES   = 3.0
JITTER_DETAILS = (0.3, 1.3)
JITTER_PAGES   = (0.2, 1.0)

DETAIL_MAX_RETRIES = 7
DETAIL_BASE_SLEEP  = 3.0

CHECKPOINT_EVERY_N = 20
MAX_DRIVER_RESTARTS = 12
""",
            language="python"
        )

    with st.expander("2) B√∫squeda con filtros (run_search)", expanded=False):
        st.code(
            """def run_search(driver, wait):
    driver.get(URL)
    form = wait.until(EC.presence_of_element_located((By.ID, "searchform")))

    Select(form.find_element(By.NAME, "yearfrom")).select_by_value(YEAR_FROM)
    Select(form.find_element(By.NAME, "yearto")).select_by_value(YEAR_TO)
    Select(form.find_element(By.NAME, "newused")).select_by_value(NEWUSED)

    btn = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "#searchform button[type='submit']")))
    driver.execute_script("arguments[0].scrollIntoView({block:'center'});", btn)
    time.sleep(0.45)

    try:
        wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "#searchform button[type='submit']")))
        btn.click()
    except ElementClickInterceptedException:
        driver.execute_script("arguments[0].click();", btn)
    except Exception:
        driver.execute_script("document.getElementById('searchform').submit();")

    wait_results_ready(driver, wait)
    return driver.current_url
""",
            language="python"
        )

    with st.expander("3) Robustez: abrir detalle en nueva pesta√±a con retry (HTTP 500 / fallos)", expanded=False):
        st.code(
            """def open_detail_in_new_tab_with_retry(driver, wait, detail_url, consecutive_bad):
    results_handle = driver.current_window_handle
    base_sleep = DETAIL_BASE_SLEEP + consecutive_bad * 1.0

    for attempt in range(1, DETAIL_MAX_RETRIES + 1):
        driver.execute_script("window.open(arguments[0], '_blank');", detail_url)
        driver.switch_to.window(driver.window_handles[-1])

        time.sleep(1.0)

        if is_500_page(driver):
            driver.close()
            driver.switch_to.window(results_handle)
            sleep_s = base_sleep * (2 ** (attempt - 1)) + random.uniform(0.2, 1.6)
            time.sleep(sleep_s)
            continue

        try:
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.carheader")))
            return True, results_handle
        except Exception:
            try:
                driver.close()
            except Exception:
                pass
            driver.switch_to.window(results_handle)

            sleep_s = base_sleep * (2 ** (attempt - 1)) + random.uniform(0.2, 1.6)
            time.sleep(sleep_s)

    return False, results_handle
""",
            language="python"
        )

st.caption("TFG: Adquisici√≥n de datos | Web scraping controlado")